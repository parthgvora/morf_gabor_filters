{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPORF Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to prove that this pure python implementation of SPORF is identical, in terms of functionality, to the one used in the SPORF paper (Tomita, Tyler M., et al. \"Sparse projection oblique randomer forests.\" Journal of Machine Learning Research 21.104 (2020): 1-39.). In order to do this, this notebook runs this implementation of SPORF on 3 different data sets: hill valley, acute inflammation task 1, and acute inflammation task 2. Cohen's Kappa (fractional decrease in error rate over the chance error rate) is the metric that is being used to compare the implementations. If this implementation has the same kappa values (for the same data sets) as the one in the SPORF paper, we can say with confidence that this implementation is accurate. The datasets used in this notebook all had kappa values of 100 Â± 0 in the SPORF paper implementation, which is also what is found when run on this SPORF implementation, as seen below. Thus, we can say with confidence that this implementation of SPORF is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.voters import TreeClassificationVoter\n",
    "from proglearn.transformers import TreeClassificationTransformer\n",
    "from proglearn.transformers import ObliqueTreeClassificationTransformer\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sporf_tutorial_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPORF\n",
    "\n",
    "## Set parameters and run on hill valley without noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 2\n",
    "density = 0.01\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\", reps, n_trees, task_num,\n",
    "                            ObliqueTreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            ObliqueTreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            ObliqueTreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (RF)\n",
    "\n",
    "Now we will run the same datasets on a base Random forest. The goal of this is to show how SPORF can clearly outperform or perform as well as the Random Forest algorithm. As seen by the results below, SPORF has a much higher kappa value, than RF, for the hill valley without noise data and has the same value for the acute inflammation data sets. Having a high kappa value is desired since as mentioned above, it is a measure of how much the error rate over the chance error rate decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on hill valley without noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  0.5409836065573771\n",
      "Accuracy after iteration  1 :  0.5901639344262295\n",
      "Accuracy after iteration  2 :  0.5901639344262295\n",
      "Accuracy after iteration  3 :  0.6885245901639344\n",
      "Accuracy after iteration  4 :  0.5245901639344263\n",
      "kappa:  17.37704918032787 , error: 5.1130724431784715\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 2\n",
    "density = 0.01\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\", reps, n_trees, task_num,\n",
    "                            TreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            TreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            TreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "From the results obtained in this notebook, it is possible to conclude that this implementation of SPORF is accurate. Furthermore, it is possible to see how SPORF can be very useful, especially in a model utilizing ensembling. It can do much better than RF on certain datasets while maintaining the high kappa values seen on the datasets that RF performed well on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (400,58) into shape (400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1d174ec8fad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_transformers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/users/jmand/Documents/NDD/ProgLearn/proglearn/progressive_learner.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, task_id, transformer_ids)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \"\"\"\n\u001b[1;32m    714\u001b[0m         return self.task_id_to_decider[task_id].predict(\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         )\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/users/jmand/Documents/NDD/ProgLearn/proglearn/deciders.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, transformer_ids)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mWhen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mvote_overall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransformer_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_overall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/users/jmand/Documents/NDD/ProgLearn/proglearn/deciders.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, transformer_ids)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mvote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mvote_per_bag_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mvote_per_transformer_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_per_bag_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_per_transformer_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/mnt/c/users/jmand/Documents/NDD/tutorial/morf_gabor_filters/test/venv/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3334\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3335\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/users/jmand/Documents/NDD/tutorial/morf_gabor_filters/test/venv/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/users/jmand/Documents/NDD/tutorial/morf_gabor_filters/test/venv/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (400,58) into shape (400)"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 1\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "sample_size = 400\n",
    "\n",
    "X_train, y_train = load_simulated_data('Orthant_train.csv')\n",
    "X_test, y_test = load_simulated_data('Orthant_test.csv')\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test))\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# print(len(np.unique(np.concatenate((y_train, y_test)))))\n",
    "# print(np.amax(np.concatenate((y_train, y_test))) + 1)\n",
    "\n",
    "kappa = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    # idx = np.random.randint(len(X_train), size=sample_size)\n",
    "    \n",
    "    # X_train = X_train[idx,:]\n",
    "    # y_train = y_train[idx]\n",
    "    \n",
    "    # X_train = X_train[:sample_size,:]\n",
    "    # y_train = y_train[:sample_size]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=sample_size, shuffle=True, stratify=y)\n",
    "    \n",
    "    X_test = X_test[:400,:]\n",
    "    y_test = y_test[:400]\n",
    "            \n",
    "    kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "    default_decider_kwargs = {\"classes\": np.arange(n_classes)}\n",
    "\n",
    "    pl = ProgressiveLearner(\n",
    "        default_transformer_class=ObliqueTreeClassificationTransformer,\n",
    "        default_transformer_kwargs=kwargs,\n",
    "        default_voter_class=TreeClassificationVoter,\n",
    "        default_voter_kwargs={},\n",
    "        default_decider_class=SimpleArgmaxAverage,\n",
    "        default_decider_kwargs=default_decider_kwargs)\n",
    "\n",
    "    pl.add_task(X_train, y_train, num_transformers=n_trees)\n",
    "\n",
    "    y_hat = pl.predict(X_test, task_id=0)\n",
    "\n",
    "    acc = np.sum(y_test == y_hat) / len(y_test)\n",
    "    print(\"Accuracy after iteration \", i, \": \", acc)\n",
    "\n",
    "    chance_pred = 1 / n_classes\n",
    "    kappa[i] = (acc - chance_pred) / (1 - chance_pred)\n",
    "\n",
    "kap = np.mean(kappa) * 100\n",
    "err = (np.std(kappa) * 100) / np.sqrt(reps)\n",
    "\n",
    "print(\"kappa: \", kap, \", error:\", err)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
