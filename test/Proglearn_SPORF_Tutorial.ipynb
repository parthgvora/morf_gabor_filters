{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPORF Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to prove that this pure python implementation of SPORF is identical, in terms of functionality, to the one used in the SPORF paper (Tomita, Tyler M., et al. \"Sparse projection oblique randomer forests.\" Journal of Machine Learning Research 21.104 (2020): 1-39.). In order to do this, this notebook runs this implementation of SPORF on 3 different data sets: hill valley, acute inflammation task 1, and acute inflammation task 2. Cohen's Kappa (fractional decrease in error rate over the chance error rate) is the metric that is being used to compare the implementations. If this implementation has the same kappa values (for the same data sets) as the one in the SPORF paper, we can say with confidence that this implementation is accurate. The datasets used in this notebook all had kappa values of 100 Â± 0 in the SPORF paper implementation, which is also what is found when run on this SPORF implementation, as seen below. Thus, we can say with confidence that this implementation of SPORF is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from proglearn.progressive_learner import ProgressiveLearner\n",
    "from proglearn.voters import TreeClassificationVoter\n",
    "from proglearn.transformers import TreeClassificationTransformer\n",
    "from proglearn.transformers import ObliqueTreeClassificationTransformer\n",
    "from proglearn.deciders import SimpleArgmaxAverage\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sporf_tutorial_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPORF\n",
    "\n",
    "## Set parameters and run on hill valley without noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 2\n",
    "density = 0.01\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\", reps, n_trees, task_num,\n",
    "                            ObliqueTreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            ObliqueTreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            ObliqueTreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (RF)\n",
    "\n",
    "Now we will run the same datasets on a base Random forest. The goal of this is to show how SPORF can clearly outperform or perform as well as the Random Forest algorithm. As seen by the results below, SPORF has a much higher kappa value, than RF, for the hill valley without noise data and has the same value for the acute inflammation data sets. Having a high kappa value is desired since as mentioned above, it is a measure of how much the error rate over the chance error rate decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on hill valley without noise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  0.5409836065573771\n",
      "Accuracy after iteration  1 :  0.5901639344262295\n",
      "Accuracy after iteration  2 :  0.5901639344262295\n",
      "Accuracy after iteration  3 :  0.6885245901639344\n",
      "Accuracy after iteration  4 :  0.5245901639344263\n",
      "kappa:  17.37704918032787 , error: 5.1130724431784715\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 2\n",
    "density = 0.01\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/hill-valley/Hill_Valley_without_noise_Training.data\", reps, n_trees, task_num,\n",
    "                            TreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 1\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            TreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters and run on acute inflammation task 2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  1.0\n",
      "Accuracy after iteration  1 :  1.0\n",
      "Accuracy after iteration  2 :  1.0\n",
      "Accuracy after iteration  3 :  1.0\n",
      "Accuracy after iteration  4 :  1.0\n",
      "kappa:  100.0 , error: 0.0\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 5\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "\n",
    "kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "kappa, err = test(\"https://archive.ics.uci.edu/ml/machine-learning-databases/acute/diagnosis.data\", reps, n_trees, task_num,\n",
    "                            TreeClassificationTransformer,\n",
    "                            kwargs)\n",
    "\n",
    "print(\"kappa: \", kappa, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "From the results obtained in this notebook, it is possible to conclude that this implementation of SPORF is accurate. Furthermore, it is possible to see how SPORF can be very useful, especially in a model utilizing ensembling. It can do much better than RF on certain datasets while maintaining the high kappa values seen on the datasets that RF performed well on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  0.8184\n",
      "Accuracy after iteration  1 :  0.7984\n",
      "Accuracy after iteration  2 :  0.7975\n",
      "Accuracy after iteration  3 :  0.7906\n",
      "Accuracy after iteration  4 :  0.8352\n",
      "Accuracy after iteration  5 :  0.82\n",
      "Accuracy after iteration  6 :  0.7774\n",
      "Accuracy after iteration  7 :  0.7925\n",
      "Accuracy after iteration  8 :  0.8323\n",
      "Accuracy after iteration  9 :  0.783\n",
      "kappa:  80.14273015873016 , error: 0.6236551094478738\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 10\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "sample_size = 2000\n",
    "\n",
    "X_train, y_train = load_simulated_data('Orthant_train.csv')\n",
    "X_test, y_test = load_simulated_data('Orthant_test.csv')\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test))\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "# print(len(np.unique(np.concatenate((y_train, y_test)))))\n",
    "# print(np.amax(np.concatenate((y_train, y_test))) + 1)\n",
    "\n",
    "kappa = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    # idx = np.random.randint(len(X_train), size=sample_size)\n",
    "    \n",
    "    # X_train = X_train[idx,:]\n",
    "    # y_train = y_train[idx]\n",
    "    \n",
    "    # X_train = X_train[:sample_size,:]\n",
    "    # y_train = y_train[:sample_size]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=sample_size, shuffle=True, stratify=y)\n",
    "    \n",
    "    # X_test = X_test[:400,:]\n",
    "    # y_test = y_test[:400]\n",
    "            \n",
    "    kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "    default_decider_kwargs = {\"classes\": np.arange(n_classes)}\n",
    "\n",
    "    pl = ProgressiveLearner(\n",
    "        default_transformer_class=ObliqueTreeClassificationTransformer,\n",
    "        default_transformer_kwargs=kwargs,\n",
    "        default_voter_class=TreeClassificationVoter,\n",
    "        default_voter_kwargs={},\n",
    "        default_decider_class=SimpleArgmaxAverage,\n",
    "        default_decider_kwargs=default_decider_kwargs)\n",
    "\n",
    "    pl.add_task(X_train, y_train, num_transformers=n_trees)\n",
    "\n",
    "    y_hat = pl.predict(X_test, task_id=0)\n",
    "\n",
    "    acc = np.sum(y_test == y_hat) / len(y_test)\n",
    "    print(\"Accuracy after iteration \", i, \": \", acc)\n",
    "\n",
    "    chance_pred = 1 / n_classes\n",
    "    kappa[i] = (acc - chance_pred) / (1 - chance_pred)\n",
    "\n",
    "kap = np.mean(kappa) * 100\n",
    "err = (np.std(kappa) * 100) / np.sqrt(reps)\n",
    "\n",
    "print(\"kappa: \", kap, \", error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  0.9647\n",
      "Accuracy after iteration  1 :  0.9802\n",
      "Accuracy after iteration  2 :  0.9735\n",
      "Accuracy after iteration  3 :  0.9584\n",
      "Accuracy after iteration  4 :  0.975\n",
      "Accuracy after iteration  5 :  0.9732\n",
      "Accuracy after iteration  6 :  0.9687\n",
      "Accuracy after iteration  7 :  0.964\n",
      "Accuracy after iteration  8 :  0.9685\n",
      "Accuracy after iteration  9 :  0.9699\n",
      "kappa:  96.91276190476191 , error: 0.1907594761142605\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.1\n",
    "reps = 10\n",
    "n_trees = 10\n",
    "task_num = 2\n",
    "sample_size = 2000\n",
    "\n",
    "X_train, y_train = load_simulated_data('Orthant_train.csv')\n",
    "X_test, y_test = load_simulated_data('Orthant_test.csv')\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test))\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "kappa = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=sample_size, shuffle=True, stratify=y)\n",
    "\n",
    "    kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "    default_decider_kwargs = {\"classes\": np.arange(n_classes)}\n",
    "\n",
    "    pl = ProgressiveLearner(\n",
    "        default_transformer_class=ObliqueTreeClassificationTransformer,\n",
    "        default_transformer_kwargs=kwargs,\n",
    "        default_voter_class=TreeClassificationVoter,\n",
    "        default_voter_kwargs={},\n",
    "        default_decider_class=SimpleArgmaxAverage,\n",
    "        default_decider_kwargs=default_decider_kwargs)\n",
    "\n",
    "    pl.add_task(X_train, y_train, num_transformers=n_trees)\n",
    "\n",
    "    y_hat = pl.predict(X_test, task_id=0)\n",
    "\n",
    "    acc = np.sum(y_test == y_hat) / len(y_test)\n",
    "    print(\"Accuracy after iteration \", i, \": \", acc)\n",
    "\n",
    "    chance_pred = 1 / n_classes\n",
    "    kappa[i] = (acc - chance_pred) / (1 - chance_pred)\n",
    "\n",
    "kap = np.mean(kappa) * 100\n",
    "err = (np.std(kappa) * 100) / np.sqrt(reps)\n",
    "\n",
    "print(\"kappa: \", kap, \", error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  0.9843\n",
      "Accuracy after iteration  1 :  0.9881\n",
      "Accuracy after iteration  2 :  0.9898\n",
      "Accuracy after iteration  3 :  0.9872\n",
      "Accuracy after iteration  4 :  0.9891\n",
      "Accuracy after iteration  5 :  0.985\n",
      "Accuracy after iteration  6 :  0.9898\n",
      "Accuracy after iteration  7 :  0.9826\n",
      "Accuracy after iteration  8 :  0.99\n",
      "Accuracy after iteration  9 :  0.9899\n",
      "kappa:  98.7382857142857 , error: 0.08255241904175008\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.1\n",
    "reps = 10\n",
    "n_trees = 20\n",
    "task_num = 2\n",
    "sample_size = 2000\n",
    "\n",
    "X_train, y_train = load_simulated_data('Orthant_train.csv')\n",
    "X_test, y_test = load_simulated_data('Orthant_test.csv')\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test))\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "kappa = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=sample_size, shuffle=True, stratify=y)\n",
    "\n",
    "    kwargs = {\"kwargs\" : {\"max_depth\" : max_depth, \"feature_combinations\" : feature_combinations, \"density\" : density}}\n",
    "\n",
    "    default_decider_kwargs = {\"classes\": np.arange(n_classes)}\n",
    "\n",
    "    pl = ProgressiveLearner(\n",
    "        default_transformer_class=ObliqueTreeClassificationTransformer,\n",
    "        default_transformer_kwargs=kwargs,\n",
    "        default_voter_class=TreeClassificationVoter,\n",
    "        default_voter_kwargs={},\n",
    "        default_decider_class=SimpleArgmaxAverage,\n",
    "        default_decider_kwargs=default_decider_kwargs)\n",
    "\n",
    "    pl.add_task(X_train, y_train, num_transformers=n_trees)\n",
    "\n",
    "    y_hat = pl.predict(X_test, task_id=0)\n",
    "\n",
    "    acc = np.sum(y_test == y_hat) / len(y_test)\n",
    "    print(\"Accuracy after iteration \", i, \": \", acc)\n",
    "\n",
    "    chance_pred = 1 / n_classes\n",
    "    kappa[i] = (acc - chance_pred) / (1 - chance_pred)\n",
    "\n",
    "kap = np.mean(kappa) * 100\n",
    "err = (np.std(kappa) * 100) / np.sqrt(reps)\n",
    "\n",
    "print(\"kappa: \", kap, \", error:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after iteration  0 :  0.994\n",
      "Accuracy after iteration  1 :  0.992\n",
      "Accuracy after iteration  2 :  0.9933\n",
      "Accuracy after iteration  3 :  0.9953\n",
      "Accuracy after iteration  4 :  0.9952\n",
      "Accuracy after iteration  5 :  0.9952\n",
      "Accuracy after iteration  6 :  0.9951\n",
      "Accuracy after iteration  7 :  0.9922\n",
      "Accuracy after iteration  8 :  0.9959\n",
      "Accuracy after iteration  9 :  0.9949\n",
      "kappa:  99.42196825396825 , error: 0.041847310304159606\n"
     ]
    }
   ],
   "source": [
    "max_depth = 10\n",
    "feature_combinations = 1.5\n",
    "density = 0.5\n",
    "reps = 10\n",
    "n_trees = 20\n",
    "task_num = 2\n",
    "sample_size = 2000\n",
    "\n",
    "X_train, y_train = load_simulated_data('Orthant_train.csv')\n",
    "X_test, y_test = load_simulated_data('Orthant_test.csv')\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test))\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "kappa = np.zeros(reps)\n",
    "for i in range(reps):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=sample_size, shuffle=True, stratify=y)\n",
    "            \n",
    "    kwargs = {\"kwargs\" : {\"max_depth\" : max_depth} }\n",
    "\n",
    "    default_decider_kwargs = {\"classes\": np.arange(n_classes)}\n",
    "\n",
    "    pl = ProgressiveLearner(\n",
    "        default_transformer_class=TreeClassificationTransformer,\n",
    "        default_transformer_kwargs=kwargs,\n",
    "        default_voter_class=TreeClassificationVoter,\n",
    "        default_voter_kwargs={},\n",
    "        default_decider_class=SimpleArgmaxAverage,\n",
    "        default_decider_kwargs=default_decider_kwargs)\n",
    "\n",
    "    pl.add_task(X_train, y_train, num_transformers=n_trees)\n",
    "\n",
    "    y_hat = pl.predict(X_test, task_id=0)\n",
    "\n",
    "    acc = np.sum(y_test == y_hat) / len(y_test)\n",
    "    print(\"Accuracy after iteration \", i, \": \", acc)\n",
    "\n",
    "    chance_pred = 1 / n_classes\n",
    "    kappa[i] = (acc - chance_pred) / (1 - chance_pred)\n",
    "\n",
    "kap = np.mean(kappa) * 100\n",
    "err = (np.std(kappa) * 100) / np.sqrt(reps)\n",
    "\n",
    "print(\"kappa: \", kap, \", error:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPORF: \n",
    "    2000: 0.08255241904175008\n",
    "    \n",
    "RF:\n",
    "    2000: 0.041847310304159606"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
